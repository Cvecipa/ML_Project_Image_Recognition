{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.image import imread\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/ML_Project_Image_Recognition/Jupyter_Notebooks\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Current working directory:\", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory set.\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/workspaces/ML_Project_Image_Recognition')\n",
    "print(\"New working directory set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspaces/ML_Project_Image_Recognition'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "work_dir = os.getcwd()\n",
    "work_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output folder: outputs/v1\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'inputs/cracks_dataset_new'\n",
    "train_path = os.path.join(data_dir, 'train')\n",
    "val_path = os.path.join(data_dir, 'val')\n",
    "test_path = os.path.join(data_dir, 'test')\n",
    "\n",
    "version = 'v1'\n",
    "output_path = os.path.join('outputs', version)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "print(f\"Output folder: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape loaded: (256, 256, 3)\n",
      "Class labels: ['Cracked', 'Non-cracked']\n"
     ]
    }
   ],
   "source": [
    "image_shape = joblib.load(f'{output_path}/image_shape.pkl')\n",
    "print(\"Image shape loaded:\", image_shape)\n",
    "\n",
    "class_indices = joblib.load(f'{output_path}/class_indices.pkl')\n",
    "labels = list(class_indices.keys())\n",
    "print(\"Class labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation \n",
    " - Augmenting only the \"Cracked\" images to create image diversity when training as the disparity between cracked and non-cracked images is quite large as noted in notebook 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_gen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "non_augmented_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def create_combined_generator(structure_name, image_shape=(256, 256), batch_size=32):\n",
    "    cracked_dir = os.path.join(train_path, structure_name, \"Cracked\")\n",
    "    non_cracked_dir = os.path.join(train_path, structure_name, \"Non-cracked\")\n",
    "\n",
    "    cracked_generator = augmented_gen.flow_from_directory(\n",
    "        directory=os.path.join(train_path, structure_name),\n",
    "        classes=[\"Cracked\"],\n",
    "        target_size=image_shape[:2],\n",
    "        class_mode='binary',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    non_cracked_generator = non_augmented_gen.flow_from_directory(\n",
    "        directory=os.path.join(train_path, structure_name),\n",
    "        classes=[\"Non-cracked\"],\n",
    "        target_size=image_shape[:2],\n",
    "        class_mode='binary',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    def combined_gen():\n",
    "        while True:\n",
    "            cracked_imgs, cracked_labels = cracked_generator.next()\n",
    "            non_imgs, non_labels = non_cracked_generator.next()\n",
    "\n",
    "            X = np.concatenate((cracked_imgs, non_imgs), axis=0)\n",
    "            y = np.concatenate((cracked_labels, non_labels), axis=0)\n",
    "\n",
    "            indices = np.arange(len(X))\n",
    "            np.random.shuffle(indices)\n",
    "            yield X[indices], y[indices]\n",
    "\n",
    "    return combined_gen(), len(cracked_generator.filenames) + len(non_cracked_generator.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2695 images belonging to 1 classes.\n",
      "Found 10000 images belonging to 1 classes.\n",
      "Found 1417 images belonging to 1 classes.\n",
      "Found 8116 images belonging to 1 classes.\n",
      "Found 1825 images belonging to 1 classes.\n",
      "Found 15208 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "walls_train_gen, walls_total_images = create_combined_generator(\"Walls\")\n",
    "decks_train_gen, decks_total_images = create_combined_generator(\"Decks\")\n",
    "pavements_train_gen, pavements_total_images = create_combined_generator(\"Pavements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling\n",
    " - Avoiding augmentation on the Test and Validation sets and opting instead for just rescaling, this is to reflect real world images when testing the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "def create_eval_generator(base_path, structure_name, batch_size=32):\n",
    "    return rescale_gen.flow_from_directory(\n",
    "        directory=os.path.join(base_path, structure_name),\n",
    "        target_size=image_shape[:2],\n",
    "        class_mode='binary',\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1813 images belonging to 2 classes.\n",
      "Found 1361 images belonging to 2 classes.\n",
      "Found 2432 images belonging to 2 classes.\n",
      "Found 3630 images belonging to 2 classes.\n",
      "Found 2726 images belonging to 2 classes.\n",
      "Found 4869 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "walls_val_gen = create_eval_generator(val_path, \"Walls\")\n",
    "decks_val_gen = create_eval_generator(val_path, \"Decks\")\n",
    "pavements_val_gen = create_eval_generator(val_path, \"Pavements\")\n",
    "\n",
    "walls_test_gen = create_eval_generator(test_path, \"Walls\")\n",
    "decks_test_gen = create_eval_generator(test_path, \"Decks\")\n",
    "pavements_test_gen = create_eval_generator(test_path, \"Pavements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_montage(generator, structure_name, output_path, n_images=9):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from math import ceil\n",
    "\n",
    "    imgs, labels = next(generator)\n",
    "    imgs = imgs[:n_images]\n",
    "\n",
    "    n_cols = 3\n",
    "    n_rows = ceil(n_images / n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, img in enumerate(imgs):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"Augmented\")\n",
    "\n",
    "    for i in range(n_images, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    filename = os.path.join(output_path, f\"augmented_samples_{structure_name}.png\")\n",
    "    plt.savefig(filename, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2695 images belonging to 1 classes.\n",
      "Saved: outputs/v1/augmented_samples_Walls.png\n",
      "Found 1417 images belonging to 1 classes.\n",
      "Saved: outputs/v1/augmented_samples_Decks.png\n",
      "Found 1825 images belonging to 1 classes.\n",
      "Saved: outputs/v1/augmented_samples_Pavements.png\n"
     ]
    }
   ],
   "source": [
    "save_augmented_montage(\n",
    "    augmented_gen.flow_from_directory(\n",
    "        directory=os.path.join(train_path, \"Walls\"),\n",
    "        classes=[\"Cracked\"],\n",
    "        target_size=image_shape[:2],\n",
    "        class_mode='binary',\n",
    "        batch_size=9,\n",
    "        shuffle=True\n",
    "    ),\n",
    "    \"Walls\", output_path\n",
    ")\n",
    "\n",
    "save_augmented_montage(\n",
    "    augmented_gen.flow_from_directory(\n",
    "        directory=os.path.join(train_path, \"Decks\"),\n",
    "        classes=[\"Cracked\"],\n",
    "        target_size=image_shape[:2],\n",
    "        class_mode='binary',\n",
    "        batch_size=9,\n",
    "        shuffle=True\n",
    "    ),\n",
    "    \"Decks\", output_path\n",
    ")\n",
    "\n",
    "save_augmented_montage(\n",
    "    augmented_gen.flow_from_directory(\n",
    "        directory=os.path.join(train_path, \"Pavements\"),\n",
    "        classes=[\"Cracked\"],\n",
    "        target_size=image_shape[:2],\n",
    "        class_mode='binary',\n",
    "        batch_size=9,\n",
    "        shuffle=True\n",
    "    ),\n",
    "    \"Pavements\", output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "montage_files = [\n",
    "    os.path.join(output_path, \"augmented_samples_Walls.png\"),\n",
    "    os.path.join(output_path, \"augmented_samples_Decks.png\"),\n",
    "    os.path.join(output_path, \"augmented_samples_Pavements.png\")\n",
    "]\n",
    "\n",
    "for file in montage_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\"Displaying: {os.path.basename(file)}\")\n",
    "        display(Image(filename=file))\n",
    "    else:\n",
    "        print(f\"File not found: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_gen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12695 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "walls_train_gen = simple_gen.flow_from_directory(\n",
    "    directory=os.path.join(train_path, \"Walls\"),\n",
    "    target_size=image_shape[:2],\n",
    "    class_mode='binary',\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "def create_tf_model_1(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.001), input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 13:47:07.392490: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "396/396 [==============================] - 1354s 3s/step - loss: 1.8689 - accuracy: 0.7711 - val_loss: 0.9085 - val_accuracy: 0.7801\n",
      "Epoch 2/25\n",
      "396/396 [==============================] - 1361s 3s/step - loss: 1.1834 - accuracy: 0.7812 - val_loss: 2.0753 - val_accuracy: 0.5675\n",
      "Epoch 3/25\n",
      "396/396 [==============================] - 1347s 3s/step - loss: 0.9196 - accuracy: 0.7874 - val_loss: 0.8041 - val_accuracy: 0.7852\n",
      "Epoch 4/25\n",
      "396/396 [==============================] - 1404s 4s/step - loss: 0.7737 - accuracy: 0.7873 - val_loss: 0.7172 - val_accuracy: 0.7852\n",
      "Epoch 5/25\n",
      "396/396 [==============================] - 1409s 4s/step - loss: 0.6914 - accuracy: 0.7878 - val_loss: 0.6606 - val_accuracy: 0.7852\n",
      "Epoch 6/25\n",
      "396/396 [==============================] - 1400s 4s/step - loss: 1.1560 - accuracy: 0.7815 - val_loss: 3.1685 - val_accuracy: 0.2171\n",
      "Epoch 7/25\n",
      "396/396 [==============================] - 1403s 4s/step - loss: 1.7360 - accuracy: 0.7868 - val_loss: 1.2538 - val_accuracy: 0.7852\n",
      "Epoch 8/25\n",
      "396/396 [==============================] - 1394s 4s/step - loss: 1.0207 - accuracy: 0.7875 - val_loss: 0.8560 - val_accuracy: 0.7852\n"
     ]
    }
   ],
   "source": [
    "tf_model_1 = create_tf_model_1(input_shape=image_shape)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history_tf_model_1 = tf_model_1.fit(\n",
    "    walls_train_gen,\n",
    "    steps_per_epoch=walls_train_gen.samples // walls_train_gen.batch_size,\n",
    "    validation_data=walls_val_gen,\n",
    "    validation_steps=walls_val_gen.samples // walls_val_gen.batch_size,\n",
    "    epochs=25,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/v1/history_tf_model_1_walls.pkl']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model_1.save(os.path.join(output_path, \"tf_model_1_walls.h5\"))\n",
    "joblib.dump(history_tf_model_1.history, os.path.join(output_path, \"history_tf_model_1_walls.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: outputs/v1/tf_model_1_walls.h5\n",
      "History saved to: outputs/v1/history_tf_model_1_walls.pkl\n"
     ]
    }
   ],
   "source": [
    "print(\"Model saved to:\", os.path.join(output_path, \"tf_model_1_walls.h5\"))\n",
    "print(\"History saved to:\", os.path.join(output_path, \"history_tf_model_1_walls.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
